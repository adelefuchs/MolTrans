{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def compute_fairness_metrics1(df, thresholds, group_column):\n",
    "    \"\"\"\n",
    "    Computes fairness metrics for different groups using group-specific thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Data containing 'True_Label' and 'Predicted_Score'.\n",
    "    - thresholds (dict): Dictionary mapping group values (e.g., 0 for DAVIS, 1 for Pharos) to their classification threshold.\n",
    "    - group_column (str): Column name specifying the group membership.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Fairness metrics including Demographic Parity, Equalized Odds, Equal Opportunity, Disparate Impact, FNR, and FOR.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply group-specific threshold to determine predicted labels\n",
    "    df['Predicted_Label'] = df.apply(lambda row: int(row['Predicted_Score'] >= thresholds[row[group_column]]), axis=1)\n",
    "\n",
    "    # Separate groups\n",
    "    group_1 = df[df[group_column] == 1]  # Pharos (understudied proteins)\n",
    "    group_0 = df[df[group_column] == 0]  # DAVIS (well-studied proteins)\n",
    "\n",
    "    # Compute Demographic Parity\n",
    "    dp_group_1 = group_1['Predicted_Label'].mean()\n",
    "    dp_group_0 = group_0['Predicted_Label'].mean()\n",
    "    demographic_parity = abs(dp_group_1 - dp_group_0)\n",
    "\n",
    "    # Compute Confusion Matrices\n",
    "    cm_1 = confusion_matrix(group_1['True_Label'], group_1['Predicted_Label'])\n",
    "    cm_0 = confusion_matrix(group_0['True_Label'], group_0['Predicted_Label'])\n",
    "\n",
    "    # Extract True Positive Rate (TPR) and False Positive Rate (FPR)\n",
    "    tpr_1 = cm_1[1, 1] / (cm_1[1, 0] + cm_1[1, 1]) if (cm_1[1, 0] + cm_1[1, 1]) > 0 else 0\n",
    "    tpr_0 = cm_0[1, 1] / (cm_0[1, 0] + cm_0[1, 1]) if (cm_0[1, 0] + cm_0[1, 1]) > 0 else 0\n",
    "\n",
    "    fpr_1 = cm_1[0, 1] / (cm_1[0, 0] + cm_1[0, 1]) if (cm_1[0, 0] + cm_1[0, 1]) > 0 else 0\n",
    "    fpr_0 = cm_0[0, 1] / (cm_0[0, 0] + cm_0[0, 1]) if (cm_0[0, 0] + cm_0[0, 1]) > 0 else 0\n",
    "\n",
    "    equalized_odds = abs(tpr_1 - tpr_0) + abs(fpr_1 - fpr_0)\n",
    "    equal_opportunity = abs(tpr_1 - tpr_0)\n",
    "\n",
    "    # Compute Disparate Impact\n",
    "    dp_impact = dp_group_1 / dp_group_0 if dp_group_0 > 0 else float('inf')\n",
    "\n",
    "    # Compute False Negative Rate (FNR) and False Omission Rate (FOR)\n",
    "    fnr_1 = cm_1[1, 0] / (cm_1[1, 0] + cm_1[1, 1]) if (cm_1[1, 0] + cm_1[1, 1]) > 0 else 0\n",
    "    fnr_0 = cm_0[1, 0] / (cm_0[1, 0] + cm_0[1, 1]) if (cm_0[1, 0] + cm_0[1, 1]) > 0 else 0\n",
    "\n",
    "    for_1 = cm_1[0, 0] / (cm_1[0, 0] + cm_1[0, 1]) if (cm_1[0, 0] + cm_1[0, 1]) > 0 else 0\n",
    "    for_0 = cm_0[0, 0] / (cm_0[0, 0] + cm_0[0, 1]) if (cm_0[0, 0] + cm_0[0, 1]) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'Demographic Parity': demographic_parity,\n",
    "        'Equalized Odds': equalized_odds,\n",
    "        'Equal Opportunity': equal_opportunity,\n",
    "        'Disparate Impact': dp_impact,\n",
    "        'False Negative Rate (Pharos)': fnr_1,\n",
    "        'False Negative Rate (DAVIS)': fnr_0,\n",
    "        'False Omission Rate (Pharos)': for_1,\n",
    "        'False Omission Rate (DAVIS)': for_0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Parity: 0.4832\n",
      "Equalized Odds: 0.4052\n",
      "Equal Opportunity: 0.0259\n",
      "Disparate Impact: 2.8537\n",
      "False Negative Rate (Pharos): 0.1186\n",
      "False Negative Rate (DAVIS): 0.0927\n",
      "False Omission Rate (Pharos): 0.3943\n",
      "False Omission Rate (DAVIS): 0.7736\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_davis = pd.read_csv(\"test_predictions_1_19.csv\")\n",
    "df_pharos = pd.read_csv(\"test_predictions_pharos (2).csv\")\n",
    "\n",
    "# Add a group column\n",
    "df_davis['Group'] = 0  # DAVIS (well-studied proteins)\n",
    "df_pharos['Group'] = 1  # PHAROS (understudied proteins)\n",
    "\n",
    "# Merge both datasets\n",
    "df = pd.concat([df_davis, df_pharos], ignore_index=True)\n",
    "\n",
    "# Define group-specific thresholds\n",
    "thresholds = {0: 0.297257661819458, 1: 0.019704099744558334}\n",
    "\n",
    "# Compute fairness metrics\n",
    "fairness_results = compute_fairness_metrics1(df, thresholds, 'Group')\n",
    "\n",
    "# Print results\n",
    "for metric, value in fairness_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-wise Metrics:\n",
      "\n",
      "DAVIS:\n",
      "  Accuracy: 0.7803\n",
      "  Precision: 0.1752\n",
      "  Recall: 0.9073\n",
      "  F1-Score: 0.2937\n",
      "  Positive Rate: 0.2607\n",
      "\n",
      "PHAROS:\n",
      "  Accuracy: 0.6383\n",
      "  Precision: 0.5937\n",
      "  Recall: 0.8814\n",
      "  F1-Score: 0.7095\n",
      "  Positive Rate: 0.7439\n",
      "\n",
      "Fairness Disparities (DAVIS - PHAROS):\n",
      "  Accuracy Gap: 0.1420\n",
      "  Precision Gap: -0.4185\n",
      "  Recall Gap: 0.0259\n",
      "  F1-Score Gap: -0.4158\n",
      "  Positive Rate Gap: -0.4832\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load datasets\n",
    "df_davis = pd.read_csv(\"test_predictions_1_19.csv\")\n",
    "df_pharos = pd.read_csv(\"test_predictions_pharos (2).csv\")\n",
    "\n",
    "# Add source column\n",
    "df_davis['Group'] = 'DAVIS'\n",
    "df_pharos['Group'] = 'PHAROS'\n",
    "\n",
    "# Merge datasets\n",
    "df = pd.concat([df_davis, df_pharos], ignore_index=True)\n",
    "\n",
    "# Define classification thresholds\n",
    "thresholds = {'DAVIS': 0.297257661819458, 'PHAROS': 0.019704099744558334}\n",
    "\n",
    "# Apply thresholds to classify predictions\n",
    "df['Predicted_Label'] = df.apply(lambda row: 1 if row['Predicted_Score'] >= thresholds[row['Group']] else 0, axis=1)\n",
    "\n",
    "# Compute fairness metrics\n",
    "def compute_fairness_metrics2(df):\n",
    "    metrics = {}\n",
    "    for group in df['Group'].unique():\n",
    "        subset = df[df['Group'] == group]\n",
    "        y_true, y_pred = subset['True_Label'], subset['Predicted_Label']\n",
    "        \n",
    "        metrics[group] = {\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'F1-Score': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'Positive Rate': y_pred.mean(),  # Proportion classified as positive\n",
    "        }\n",
    "    \n",
    "    # Compute fairness disparities\n",
    "    davis_metrics, pharos_metrics = metrics['DAVIS'], metrics['PHAROS']\n",
    "    fairness_disparities = {\n",
    "        'Accuracy Gap': davis_metrics['Accuracy'] - pharos_metrics['Accuracy'],\n",
    "        'Precision Gap': davis_metrics['Precision'] - pharos_metrics['Precision'],\n",
    "        'Recall Gap': davis_metrics['Recall'] - pharos_metrics['Recall'],\n",
    "        'F1-Score Gap': davis_metrics['F1-Score'] - pharos_metrics['F1-Score'],\n",
    "        'Positive Rate Gap': davis_metrics['Positive Rate'] - pharos_metrics['Positive Rate'],\n",
    "    }\n",
    "    \n",
    "    return metrics, fairness_disparities\n",
    "\n",
    "metrics, disparities = compute_fairness_metrics2(df)\n",
    "\n",
    "# Print results\n",
    "print(\"Group-wise Metrics:\")\n",
    "for group, metric_vals in metrics.items():\n",
    "    print(f\"\\n{group}:\")\n",
    "    for metric, value in metric_vals.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nFairness Disparities (DAVIS - PHAROS):\")\n",
    "for metric, value in disparities.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Parity: 0.2464\n",
      "Equalized Odds: 0.1034\n",
      "Equal Opportunity: 0.0931\n",
      "Disparate Impact: 1.5333\n",
      "False Negative Rate (Pharos): 0.2346\n",
      "False Negative Rate (DAVIS): 0.1415\n",
      "False Omission Rate (Pharos): 0.6000\n",
      "False Omission Rate (DAVIS): 0.5897\n",
      "Group-wise Metrics:\n",
      "\n",
      "DAVIS:\n",
      "  Accuracy: 0.6207\n",
      "  Precision: 0.2141\n",
      "  Recall: 0.8585\n",
      "  F1-Score: 0.3427\n",
      "  Positive Rate: 0.4620\n",
      "\n",
      "PHAROS:\n",
      "  Accuracy: 0.7396\n",
      "  Precision: 0.9118\n",
      "  Recall: 0.7654\n",
      "  F1-Score: 0.8322\n",
      "  Positive Rate: 0.7083\n",
      "\n",
      "Fairness Disparities (DAVIS - PHAROS):\n",
      "  Accuracy Gap: -0.1189\n",
      "  Precision Gap: -0.6976\n",
      "  Recall Gap: 0.0931\n",
      "  F1-Score Gap: -0.4895\n",
      "  Positive Rate Gap: -0.2464\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_davis = pd.read_csv(\"test_predictions_b3.csv\")\n",
    "df_pharos = pd.read_csv(\"test_predictions_pharos_allp (2).csv\")\n",
    "\n",
    "# Add a group column\n",
    "df_davis['Group'] = 0  # DAVIS (well-studied proteins)\n",
    "df_pharos['Group'] = 1  # PHAROS (understudied proteins)\n",
    "\n",
    "# Merge both datasets\n",
    "df = pd.concat([df_davis, df_pharos], ignore_index=True)\n",
    "\n",
    "# Define group-specific thresholds\n",
    "thresholds = {0: 0.007697489112615585, 1: 0.025588534772396088}\n",
    "\n",
    "# Compute fairness metrics\n",
    "fairness_results = compute_fairness_metrics1(df, thresholds, 'Group')\n",
    "\n",
    "# Print results\n",
    "for metric, value in fairness_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Add source column\n",
    "df_davis['Group'] = 'DAVIS'\n",
    "df_pharos['Group'] = 'PHAROS'\n",
    "\n",
    "# Merge datasets\n",
    "df = pd.concat([df_davis, df_pharos], ignore_index=True)\n",
    "\n",
    "# Define classification thresholds\n",
    "thresholds = {'DAVIS': 0.007697489112615585\n",
    ", 'PHAROS': 0.025588534772396088}\n",
    "\n",
    "# Apply thresholds to classify predictions\n",
    "df['Predicted_Label'] = df.apply(lambda row: 1 if row['Predicted_Score'] >= thresholds[row['Group']] else 0, axis=1)\n",
    "\n",
    "metrics, disparities = compute_fairness_metrics2(df)\n",
    "\n",
    "# Print results\n",
    "print(\"Group-wise Metrics:\")\n",
    "for group, metric_vals in metrics.items():\n",
    "    print(f\"\\n{group}:\")\n",
    "    for metric, value in metric_vals.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nFairness Disparities (DAVIS - PHAROS):\")\n",
    "for metric, value in disparities.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Parity: 0.2819\n",
      "Equalized Odds: 0.2183\n",
      "Equal Opportunity: 0.0229\n",
      "Disparate Impact: 1.6102\n",
      "False Negative Rate (Pharos): 0.1186\n",
      "False Negative Rate (DAVIS): 0.1415\n",
      "False Omission Rate (Pharos): 0.3943\n",
      "False Omission Rate (DAVIS): 0.5897\n",
      "Group-wise Metrics:\n",
      "\n",
      "DAVIS:\n",
      "  Accuracy: 0.6207\n",
      "  Precision: 0.2141\n",
      "  Recall: 0.8585\n",
      "  F1-Score: 0.3427\n",
      "  Positive Rate: 0.4620\n",
      "\n",
      "PHAROS:\n",
      "  Accuracy: 0.6383\n",
      "  Precision: 0.5937\n",
      "  Recall: 0.8814\n",
      "  F1-Score: 0.7095\n",
      "  Positive Rate: 0.7439\n",
      "\n",
      "Fairness Disparities (DAVIS - PHAROS):\n",
      "  Accuracy Gap: -0.0177\n",
      "  Precision Gap: -0.3795\n",
      "  Recall Gap: -0.0229\n",
      "  F1-Score Gap: -0.3667\n",
      "  Positive Rate Gap: -0.2819\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_davis = pd.read_csv(\"test_predictions_b3.csv\")\n",
    "df_pharos = pd.read_csv(\"test_predictions_pharos (2).csv\")\n",
    "\n",
    "# Add a group column\n",
    "df_davis['Group'] = 0  # DAVIS (well-studied proteins)\n",
    "df_pharos['Group'] = 1  # PHAROS (understudied proteins)\n",
    "\n",
    "# Merge both datasets\n",
    "df = pd.concat([df_davis, df_pharos], ignore_index=True)\n",
    "\n",
    "# Define group-specific thresholds\n",
    "thresholds = {0: 0.007697489112615585, 1: 0.019704099744558334}\n",
    "\n",
    "# Compute fairness metrics\n",
    "fairness_results = compute_fairness_metrics1(df, thresholds, 'Group')\n",
    "\n",
    "# Print results\n",
    "for metric, value in fairness_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Add source column\n",
    "df_davis['Group'] = 'DAVIS'\n",
    "df_pharos['Group'] = 'PHAROS'\n",
    "\n",
    "# Merge datasets\n",
    "df = pd.concat([df_davis, df_pharos], ignore_index=True)\n",
    "\n",
    "# Define classification thresholds\n",
    "thresholds = {'DAVIS': 0.007697489112615585\n",
    ", 'PHAROS': 0.019704099744558334}\n",
    "\n",
    "# Apply thresholds to classify predictions\n",
    "df['Predicted_Label'] = df.apply(lambda row: 1 if row['Predicted_Score'] >= thresholds[row['Group']] else 0, axis=1)\n",
    "\n",
    "metrics, disparities = compute_fairness_metrics2(df)\n",
    "\n",
    "# Print results\n",
    "print(\"Group-wise Metrics:\")\n",
    "for group, metric_vals in metrics.items():\n",
    "    print(f\"\\n{group}:\")\n",
    "    for metric, value in metric_vals.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nFairness Disparities (DAVIS - PHAROS):\")\n",
    "for metric, value in disparities.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
